# analytics.py
"""
M√≥dulo simples para visualiza√ß√£o de gr√°ficos em janelas ap√≥s simula√ß√µes
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from typing import List
import sys

class VisualizadorResultados:
    """Visualiza resultados das simula√ß√µes em janelas gr√°ficas"""

    def __init__(self):
        plt.style.use('seaborn-v0_8-darkgrid')
        self.figuras = []

    def mostrar_menu(self) -> str:
        """Mostra menu simples de op√ß√µes"""
        print("\n" + "="*60)
        print("üìä VISUALIZA√á√ÉO DE RESULTADOS")
        print("="*60)
        print("\nEscolha os gr√°ficos a visualizar:")
        print("  1. Curva de Aprendizagem (evolu√ß√£o por epis√≥dio)")
        print("  2. Compara√ß√£o entre Agentes")
        print("  3. Evolu√ß√£o do Epsilon (Q-Learning)")
        print("  4. Todos os gr√°ficos em sequ√™ncia")
        print("  5. Sair")

        return input("\nOp√ß√£o (1-5): ").strip()

    def plotar_curva_aprendizagem(self, motor):
        """Mostra curva de aprendizagem"""
        if not hasattr(motor, 'historico_episodios') or not motor.historico_episodios:
            print("‚ö†Ô∏è Sem dados de epis√≥dios para mostrar curva de aprendizagem")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        fig.suptitle('üìà CURVA DE APRENDIZAGEM', fontsize=14, fontweight='bold')

        # Dados dos epis√≥dios
        episodios = motor.historico_episodios
        recompensas = [e['recompensa_total'] for e in episodios]
        passos = [e['passos'] for e in episodios]

        # Gr√°fico 1: Recompensa
        ax1.plot(range(len(recompensas)), recompensas, 'b-', linewidth=2, marker='o', markersize=4)
        ax1.set_xlabel('Epis√≥dio')
        ax1.set_ylabel('Recompensa Total')
        ax1.set_title('Recompensa por Epis√≥dio')
        ax1.grid(True, alpha=0.3)

        # Linha de m√©dia m√≥vel
        if len(recompensas) > 5:
            media_movel = pd.Series(recompensas).rolling(5).mean()
            ax1.plot(range(len(media_movel)), media_movel, 'r--', linewidth=2, label='M√©dia (5)')
            ax1.legend()

        # Gr√°fico 2: Passos
        ax2.plot(range(len(passos)), passos, 'g-', linewidth=2, marker='s', markersize=4)
        ax2.set_xlabel('Epis√≥dio')
        ax2.set_ylabel('Passos')
        ax2.set_title('Passos por Epis√≥dio')
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plotar_comparacao_agentes(self, motor):
        """Compara desempenho dos agentes baseado no modo de opera√ß√£o"""
        if not motor.agentes:
            print("‚ö†Ô∏è Nenhum agente para comparar")
            return

        # Determinar modo de opera√ß√£o
        modo_operacao = getattr(motor, 'modo_operacao', 'teste').lower()
        
        # Filtrar agentes baseado no modo
        agentes_filtrados = []
        tipos_esperados = []
        
        if modo_operacao == 'aprendizagem':
            # Modo aprendizagem: Gen√©tico vs Q-Learning
            tipos_esperados = ['Gen√©tico', 'Q-Learning']
            for agente in motor.agentes:
                tipo_classe = type(agente).__name__
                if ('Evolucionario' in tipo_classe or 'Genetico' in tipo_classe or 
                    'QLearning' in tipo_classe or 'Q-Learning' in tipo_classe):
                    agentes_filtrados.append(agente)
            titulo = 'üë• COMPARA√á√ÉO: GEN√âTICO vs Q-LEARNING (Modo Aprendizagem)'
        else:
            # Modo teste: Reativo vs Q-Learning (EXCLUIR Gen√©tico explicitamente)
            tipos_esperados = ['Reativo', 'Q-Learning']
            for agente in motor.agentes:
                tipo_classe = type(agente).__name__
                
                # EXCLUIR explicitamente agentes gen√©ticos
                is_genetico = ('Evolucionario' in tipo_classe or 
                              'Genetico' in tipo_classe or
                              'genetico' in tipo_classe.lower() or
                              'Evolucion√°rio' in tipo_classe)
                
                if is_genetico:
                    continue  # Pular agentes gen√©ticos no modo teste
                
                # Verificar se √© Reativo ou Q-Learning (mais robusto)
                is_reativo = ('Reativo' in tipo_classe or 
                             'reativo' in tipo_classe.lower() or
                             'AgenteReativo' in tipo_classe)
                is_qlearning = ('QLearning' in tipo_classe or 
                               'Q-Learning' in tipo_classe or
                               'Q_Learning' in tipo_classe or
                               'AgenteQLearning' in tipo_classe)
                
                if is_reativo or is_qlearning:
                    agentes_filtrados.append(agente)
            titulo = 'üë• COMPARA√á√ÉO: REATIVO vs Q-LEARNING (Modo Teste)'
        
        if not agentes_filtrados:
            print(f"‚ö†Ô∏è Nenhum agente {', '.join(tipos_esperados)} encontrado para comparar")
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(titulo, fontsize=14, fontweight='bold')

        # Coletar dados b√°sicos apenas dos agentes filtrados
        dados = []
        for agente in agentes_filtrados:
            stats = agente.obter_estatisticas()
            tipo_classe = type(agente).__name__
            
            # Nome mais amig√°vel para o tipo (mais robusto)
            tipo_nome = tipo_classe
            
            # No modo teste, garantir que apenas Reativo e Q-Learning sejam mapeados
            if modo_operacao != 'aprendizagem':
                # Modo teste: apenas Reativo ou Q-Learning
                if 'Reativo' in tipo_classe or 'reativo' in tipo_classe.lower():
                    tipo_nome = 'Reativo'
                elif 'QLearning' in tipo_classe or 'Q-Learning' in tipo_classe or 'qlearning' in tipo_classe.lower():
                    tipo_nome = 'Q-Learning'
                else:
                    # Se n√£o for Reativo nem Q-Learning, pular este agente
                    continue
            else:
                # Modo aprendizagem: Gen√©tico ou Q-Learning
                if 'Evolucionario' in tipo_classe or 'Genetico' in tipo_classe or 'genetico' in tipo_classe.lower():
                    tipo_nome = 'Gen√©tico'
                elif 'QLearning' in tipo_classe or 'Q-Learning' in tipo_classe or 'qlearning' in tipo_classe.lower():
                    tipo_nome = 'Q-Learning'
                else:
                    continue
            
            dados.append({
                'id': agente.agente_id,
                'tipo': tipo_nome,
                'recompensa': stats['recompensa_acumulada'],
                'explorados': stats['espacos_explorados'],
                'passos': stats['num_acoes']
            })

        # Agrupar por tipo para melhor visualiza√ß√£o
        tipos = [d['tipo'] for d in dados]
        ids = [d['id'] for d in dados]
        recompensas = [d['recompensa'] for d in dados]
        explorados = [d['explorados'] for d in dados]
        
        # Cores diferentes por tipo e modo
        if modo_operacao == 'aprendizagem':
            # Gen√©tico (laranja) vs Q-Learning (azul)
            cores = ['#FF9800' if t == 'Gen√©tico' else '#2196F3' for t in tipos]
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor='#FF9800', edgecolor='black', label='Gen√©tico'),
                Patch(facecolor='#2196F3', edgecolor='black', label='Q-Learning')
            ]
        else:
            # Reativo (verde) vs Q-Learning (azul)
            cores = ['#4CAF50' if t == 'Reativo' else '#2196F3' for t in tipos]
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor='#4CAF50', edgecolor='black', label='Reativo'),
                Patch(facecolor='#2196F3', edgecolor='black', label='Q-Learning')
            ]

        # Gr√°fico 1: Recompensas
        bars1 = ax1.bar(ids, recompensas, color=cores, edgecolor='black')
        ax1.set_xlabel('Agente')
        ax1.set_ylabel('Recompensa')
        ax1.set_title('Recompensa Total')
        ax1.tick_params(axis='x', rotation=45)
        ax1.legend(handles=legend_elements, loc='upper right')

        # Valores nas barras
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                     f'{height:.1f}', ha='center', va='bottom')

        # Gr√°fico 2: Explora√ß√£o
        bars2 = ax2.bar(ids, explorados, color=cores, edgecolor='black')
        ax2.set_xlabel('Agente')
        ax2.set_ylabel('Espa√ßos Explorados')
        ax2.set_title('Capacidade de Explora√ß√£o')
        ax2.tick_params(axis='x', rotation=45)
        ax2.legend(handles=legend_elements, loc='upper right')

        # Gr√°fico 3: Efici√™ncia (recompensa/passo)
        eficiencias = [d['recompensa']/max(d['passos'], 1) for d in dados]
        bars3 = ax3.bar(ids, eficiencias, color=cores, edgecolor='black')
        ax3.set_xlabel('Agente')
        ax3.set_ylabel('Recompensa/Passo')
        ax3.set_title('Efici√™ncia')
        ax3.tick_params(axis='x', rotation=45)
        ax3.legend(handles=legend_elements, loc='upper right')

        # Gr√°fico 4: Scatter plot com cores por tipo
        tipos_unicos = list(set(tipos))
        for tipo in tipos_unicos:
            # No modo teste, garantir que apenas Reativo e Q-Learning apare√ßam
            if modo_operacao != 'aprendizagem' and tipo == 'Gen√©tico':
                continue  # Pular gen√©ticos no modo teste
            
            indices = [i for i, t in enumerate(tipos) if t == tipo]
            x_vals = [explorados[i] for i in indices]
            y_vals = [recompensas[i] for i in indices]
            id_vals = [ids[i] for i in indices]
            # Cor baseada no modo
            if modo_operacao == 'aprendizagem':
                cor = '#FF9800' if tipo == 'Gen√©tico' else '#2196F3'
            else:
                # Modo teste: apenas Reativo (verde) e Q-Learning (azul)
                cor = '#4CAF50' if tipo == 'Reativo' else '#2196F3'
            ax4.scatter(x_vals, y_vals, s=150, alpha=0.7, edgecolors='black', color=cor, label=tipo)
            for x, y, id_ in zip(x_vals, y_vals, id_vals):
                ax4.annotate(id_, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=9)
        ax4.set_xlabel('Espa√ßos Explorados')
        ax4.set_ylabel('Recompensa Total')
        ax4.set_title('Explora√ß√£o vs Recompensa')
        ax4.grid(True, alpha=0.3)
        ax4.legend(loc='best')

        plt.tight_layout()
        plt.show()

    def plotar_evolucao_epsilon(self, motor):
        """Mostra evolu√ß√£o do epsilon para Q-Learning"""
        q_agents = [a for a in motor.agentes if hasattr(a, 'epsilon')]

        if not q_agents:
            print("‚ö†Ô∏è Nenhum agente Q-Learning encontrado")
            return

        fig, ax = plt.subplots(figsize=(10, 6))

        for agente in q_agents:
            # Simular decaimento (em caso real, teria hist√≥rico)
            episodios = range(1, 101)
            epsilon_vals = [agente.epsilon * (0.995 ** ep) for ep in episodios]
            ax.plot(episodios, epsilon_vals, label=agente.agente_id, linewidth=2)

        ax.set_xlabel('Epis√≥dio')
        ax.set_ylabel('Valor do Epsilon (Œµ)')
        ax.set_title('üìâ Decaimento do Epsilon - Estrat√©gia Œµ-greedy')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plotar_todos(self, motor):
        """Mostra todos os gr√°ficos em sequ√™ncia"""
        input("\nüìà Pressione Enter para ver Curva de Aprendizagem...")
        self.plotar_curva_aprendizagem(motor)

        input("\nüë• Pressione Enter para ver Compara√ß√£o de Agentes...")
        self.plotar_comparacao_agentes(motor)

        # Verificar se tem agentes Q-Learning
        q_agents = [a for a in motor.agentes if hasattr(a, 'epsilon')]
        if q_agents:
            input("\nüìâ Pressione Enter para ver Evolu√ß√£o do Epsilon...")
            self.plotar_evolucao_epsilon(motor)

        print("\n‚úÖ Visualiza√ß√£o conclu√≠da!")